---
title: "Lab 2"
author: "Gabriela Cohen Hadid"
output: html_document
---

## *Lab 2: Sampling, Data Wrangling and Visualization*  
<br/><br/>  

Required Libraries:
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(ggplot2)
library(dplyr)
library(scales)   # needed for formatting y-axis labels to non-scientific type
library(tidyr)
library(tidyverse)
library(reshape2) # melt
library(ggthemes)
library(modelr)
############################################
library(grid) # geom_segment
library(ggrepel) # Use ggrepel::geom_label_repel
library(rvest)

# New Libraries 
library(zoo)  # To replace NAs with latest non-NA value in question 2.4
library(httr)  # To make HTTP requests and retrieve the HTML content of web pages in question 2.5
library(stringi) # To transliterate the text to ASCII characters in question 2.5


options("scipen"=100, "digits"=4)  # avoid scientific display of digits. Take 4 digits. 
```

<br/><br/>


## Q1. Scientists
![Scientists](https://s3.amazonaws.com/images.powershow.com/P1254325962eFyXl.pr.jpg)

In this question we extract and analyze text from Wikipedia describing notable female scientists from the 20th century. 

1. Use the  `rvest` library to scrape all the **names** of notable female scientists of the 20th century from 
[here](https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century). For ease of extraction, you can extract only scientists with known birth and/or death year. 
You should end up with a `names` vector of at least `500` elements, where each element is a name of a different female scientist. Print the first and last $5$ names. 


2. Create a data-frame with one row per scientist, with separate columns indicating the name, 
the `birth` year, the `death` year (leave as `NA` when one or both of them are not available), 
the nationality, and the occupation (the last two usually indicated right after the year for most scientists). 
For example, for the first scientist `Heloísa Alberto Torres` the birth year is `1895`, the death year is `1977`, 
the nationality is `Brazilian` and the occupation is `anthropologist`. 
Display the top-5 and bottom-5 of the resulting data-frame. <br>
**Notes:** a. A few scientists appear more than once, in different fields. In these cases keep the scientists as separate cases. <br>
b. The text describing some scientists may be missing and/or no in the common format shared by most scientist. 
It is ok if your code misses/gives wrong answers to some of them and you don't need to handle every special case. 
Make sure that your code gives the correct information for at least `400` of the scientists for each column. 



3. When you click on each scientist name, you are transferred into a different url containing text about this scientist. 
For example, clicking on the first name `Heloísa Alberto Torres`, brings you [here](https://en.wikipedia.org/wiki/Helo%C3%ADsa_Alberto_Torres).
Parse the data and create a new column called `urls` containing the url for each scientist. 
You may need to modify the names to get the exact urls. 
You don't have to be perfect here, and it is enough to get the correct urls for at least $400$ out of the $>500$ scientists.   <br>
In addition, the scientists are divided into fourteen fields the field of study (e.g. `Anthropology`, `Archaeology`, ...). 
Add a column indicating the field of study for each scientists. 
Extract and show the sub-table with the first scientists in each field (showing all columns for these scientistis)




4. Next we would like to retrieve the actual texts about each scientist. 
Write a function called `wiki_text_parser` that given a specific scientist's unparsed html page text as input, 
outputs the parsed biographical text as a string. <br>
The text should start at the line below the line `From Wikipedia, the free encyclopedia` in the Wikipedia page. <br>
The text should end right before the `References` of the Wikipedia page. See for example the highlighted text below. <br>
Run the function on the first name `Heloísa Alberto Torres` and verify that the biographical text is extracted correctly. 
Print the resulting text and its length in characters. <br>
**Hint:** You can look at occurrences of the scientist name




5. Retrieve `all` the parsed scientists biographies into a vector of strings called `bios`. You can use your function from the previous questions  <br>
Add the biography length in characters as a new column to the scientists data-frame. 
Find the scientist with the **shortest** and with the **longest** biography for **each** of the fourteen research fields (in terms of total number of English characters), and show them in two separate tables/dataframes. <br>
**Note:** reading all biographies may take a few minutes. <br>
Some errors may occur, but make sure that your pages urls (part b.) match and retrieve 
successfully at least $400$ out of the $>500$ biographies. <br>
**Hint:** You can use the `try` command to run another command such that if the command fail the program continues and is not stopped. 


6. Concatenate all biographies and compute the frequency $n_i$ of each of the $26$ letters in the English alphabet in the combined text. <br>
Consider uppercase and lowercase as the same letter. <br> 
Plot the sorted frequencies after normalization $p_i = n_i / n$ where $n$ it the total number of letters, in a bar-plot 



**Solutions here:** <br>

Q1.1. 
```{r, cache=TRUE} 

# Read and Extract Names Text
fem <- read_html("https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century")
all.names <- html_nodes(fem,"li")
names <- as.data.frame(html_text(all.names))
colnames(names) <- c("Names")

# Manual Adjusts 
pattern <- "\\w+\\s+\\([^\\)]+\\)(\\s+\\w+)?"  # Gets names that have a "name (date) name/nothing" format
filtered_names <- subset(names,grepl(pattern,Names)) 

last_name <- which(filtered_names$Names=="Catherine G. Wolf (1947–2018), American psychologist and expert in human-computer interaction")  # Index of last name

# Remove irrelevant rows that come after last name
filtered_names<- as.data.frame(filtered_names[1:last_name,])
colnames(filtered_names) <- c("Names")

just_names <- filtered_names
just_names$Names <- gsub("\\(.*","", just_names$Names)  # Remove everything that comes after parenthesis(including)

# Display 5 first and last names
head(just_names,5)
tail(just_names,5)

```
## 1.1 Analysis

Here we extracted the scientists from the Wikipedia page with a known birth and death year and showed the first 5 and the last 5.

We use the following manipulation the get this :

The text content - of the <li> elements is extracted using the html_text() function 
and stored in a data frame called names.
The column in the names data frame is named "Names" using the - col names() function.
The variable pattern is defined to match names that have a "name (date) name/nothing" format.
The subset() function is used to filter out rows in the names data frame that match the defined pattern, and the filtered data is stored in the filtered_names data frame.

The index of the last name in the filtered_names data frame is obtained using the which() function and stored in the last_name variable.

Rows in the filtered_names data frame that come after the last name are removed using indexing, and the filtered data is stored back in the filtered_names data frame.

The column in the filtered_names data frame is named "Names" using the col names() function.

The parentheses and everything that comes after them are removed from the names using the gsub() function with a regular expression pattern, and the modified names are 





Q1.2


```{r, cache=TRUE} 

# Adjusts

filtered_names <- filtered_names %>%
  mutate(Date = str_extract(Names, "(?<=\\()[^()]*[0-9][^()]*"),  # Extract (Date) Format
         Nationality = str_extract(Names, "(?<=\\)\\s?,\\s?).*"))  # Extract everything that comes after ")," - optional comma
filtered_names$Nationality <- gsub("^\\s+", "", as.character(filtered_names$Nationality))  # Gets rid off whitespace characters
filtered_names <- filtered_names %>%
  mutate(Occupation = str_extract(Nationality, "(?<=\\s)[a-z]{6,}")) # Gets word after space that starts with lower-case letter for occupation and minimum of 6 letters
filtered_names$Occupation <- gsub("^\\s+|\\s+$|,", "", as.character(filtered_names$Occupation))  # Gets rid off leading whitespace characters, trailing whitespace characters, or an empty string 
filtered_names <- filtered_names %>% 
  mutate(Nationality = str_extract(Nationality,"\\b[A-Z][A-Za-z-]+\\b"))  # Captures upper-case letters followed by upper case or hiphens
filtered_names <- filtered_names %>%
  mutate(BirthYear = str_extract(Date,"\\d{4}"),  # 4 first digits
         DeathYear = str_extract(Date, "(?<=–\\s?)\\d{4}"))  # Digits after the dash
filtered_names$Names <- gsub("\\(.*","", filtered_names$Names)  # Gets rid of everything after the parenthesis - including 
 # Captures string until first comma

filtered_names <- filtered_names %>% dplyr::select(-Date)  # Deletes extra column

filtered_names <- filtered_names %>%
  mutate_all(~gsub("^\\s+", "", .)) # Applies for all columns gsub function that eliminates whitespace characters in the beggining

filtered_names <- filtered_names %>%
  mutate_all(~gsub("\\s+$", "", .))  # Applies for all columns gsub function that eliminates whitespace characters in the end

head(filtered_names,5)
tail(filtered_names,5)

```



## 1.2 Analysis

These steps involved string manipulation using regular expressions and the g sub function to remove or extract 
specific patterns in the data frame columns. 
The goal was to clean and transform the data 
from the Wikipedia to have the desired format 
for further analysis. 
In the code, we transformed the scientists information from the Wikipedia into a data frame with columns 
that contain their information, such as the column Nationality and Date of Birth.

Q1.3. 
```{r, cache=TRUE} 

filtered_names <- filtered_names %>%
  mutate(link = str_replace_all(Names, "\\s+", "_"))  # Adjust format to "FirstName-LastNames" 

# Decode the URLs before encoding
filtered_names$link <- URLdecode(filtered_names$link)

# Encode the URLs after decoding
filtered_names$link <- URLencode(filtered_names$link)

hyperlinks <- html_attr(html_nodes(fem, "a"), "href")  # Gets all hyperlinks from wikipedia page

matching_links <- hyperlinks[str_detect(hyperlinks, paste(filtered_names$link, collapse = "|")) & !str_detect(hyperlinks, "File:")]  # Extracts hyperlinks if contains the strings in the link column

filtered_names <- filtered_names %>%
  mutate(urlname = NA)  # Create a new column for storing the URLs

# Matches url to name 

for (i in seq_along(filtered_names$link)) {
  regex_pattern <- paste0("^.*", filtered_names$link[i], ".*$")
  matching_indices <- which(str_detect(matching_links, regex(regex_pattern, ignore_case = TRUE)))

  if (length(matching_indices) > 0) {
    matching_url <- matching_links[matching_indices[1]]
    filtered_names$urlname[i] <- paste0("https://en.wikipedia.org", matching_url)
  } 
}

filtered_names <- filtered_names %>% dplyr::select(-link)  # Deleting link column


# Extract the subtitles (field of study)
subheadings <- html_text(html_nodes(fem, "h2 .mw-headline"))[1:14]

# Initialize an empty vector to store the field of study for each scientist
field_of_study <- character()

# Loop through each field of study and extract the first scientist's name
for (i in 1:length(subheadings)) {
  current_heading <- subheadings[i]
  
  # Find the section containing the current field of study
  section <- html_nodes(fem, xpath = paste0("//span[contains(text(), '", current_heading, "')]/parent::*/following::ul[1]"))
  
  # Extract the first scientist's name in the current field of study
  scientist_name <- html_text(html_nodes(section, "li"))[1]
  
  # Assign the scientist's name to the field of study
  field_of_study <- c(field_of_study, scientist_name)
}

field_of_study <- as.data.frame(field_of_study)
field_of_study$field_of_study <- gsub("\\(.*","", field_of_study$field_of_study)
field_of_study$field_of_study <- gsub("\\s+$", "", field_of_study$field_of_study)

filtered_names$Field <- NA

for (i in 1:length(subheadings)) {
  found_match <- FALSE  # Variable to track if a match is found
  
  for (j in 1:length(filtered_names$Names)) {
    if (filtered_names$Names[j] == field_of_study$field_of_study[i]) {
      filtered_names$Field[j] <- subheadings[i]
      found_match <- TRUE  # Set found_match to TRUE when a match is found
      break  # Break the inner loop when a match is found
    }
  }
}

bz <- zoo(filtered_names$Field)

# Replace NA values with the previous non-NA value
filtered_names$Field <- na.locf(bz, fromLast = FALSE, na.rm = FALSE)


sub_table <- filtered_names %>% group_by(Field) %>% slice(1)
sub_table

```


## 1.3 Analysis

In the first part of the code, we assigned the name of each scientist to the url containing text about this scientist. In the second part, the code performs data cleaning, string matching, and data aggregation to extract and organize information from the webpage and update the filtered_names data frame and add a new Field Column. The resulting sub_table data frame contains the first scientist's name for each unique field of study.




Q1.4. 
```{r, cache=TRUE}

wiki_text_parser <- function(html_text) {
  # Extract the biographical text based on the specified criteria
  start_index <- regexpr("From Wikipedia, the free encyclopedia", html_text, ignore.case = TRUE)
  end_index <- regexpr("<span class=\"mw-headline\" id=\"References\">References</span>", html_text, ignore.case = TRUE)
  
  # Adjust the start and end indices to exclude the marker lines
  start_index <- start_index + attr(start_index, "match.length")
  end_index <- end_index - 1
  
  # Extract the biographical text using the adjusted indices
  biographical_text <- substr(html_text, start_index, end_index)
  
  # Create a temporary XML document from the biographical text
  temp_xml <- read_html(paste("<html><body>", biographical_text, "</body></html>", sep = ""))
  
  # Convert the HTML text to normal text
  biographical_text <- html_text(temp_xml, trim = TRUE)
  
  # Return the parsed biographical text and its length
  return(list(text = biographical_text, length = nchar(stri_trans_general(biographical_text, "Latin-ASCII"))))
}

get_html_text <- function(url) {
  response <- GET(url)
  
  # Extract the HTML content from the response
  html_text <- content(response, "text")
  
  return(html_text)
}

# Example usage: Get the unparsed HTML page text for scientist "Heloísa Alberto Torres"
scientist_url <- "https://en.wikipedia.org/wiki/Helo%C3%ADsa_Alberto_Torres"
html_text <- get_html_text(scientist_url)

biographical_text <- wiki_text_parser(html_text)

# Print the resulting text and its length in characters
cat("Biographical Text:\n", biographical_text$text, "\n")



```



## 1.4 Analysis



The code defines a function called wiki_text_parser that takes an HTML text as input and extracts the biographical text from a Wikipedia page. The code also defines a function called get_html_text that takes a URL as input and retrieves the HTML content of the webpage using the GET function from the httr package.

Lastly, the code demonstrates an example usage by retrieving the HTML text of a Wikipedia page for scientist "Heloísa Alberto Torres" using the get_html_text function, and then parsing the biographical text from the HTML text using the wiki_text_parser function. The resulting biographical text and its length in characters are printed using cat.


Q1.5. 
```{r, cache=TRUE} 

url_scientist <- filtered_names$urlname

bios <- vector("list", length(url_scientist))
len_bios <- vector("list", length(url_scientist))

for (i in 1:length(url_scientist)) {
  result <- try({
    # Get the HTML text of the scientist's Wikipedia page
    html_text <- get_html_text(url_scientist[i])
    
    # Parse the biographical text
    biography_info <- wiki_text_parser(html_text)
    
    # Store the parsed biography text in the vector
    bios[i] <- biography_info$text
    len_bios[i] <- biography_info$length
    
  })
  
  # Check if an error occurred
  if (inherits(result, "try-error")) {
    bios[i] <- NA
    len_bios[i] <- NA
  }
}

# Convert the bios list to a character vector
bios <- unlist(bios)
len_bios <- unlist(len_bios)

temp_df <- as.data.frame(len_bios)

# Replace empty cells with NA
temp_df[temp_df == ""] <- NA

# Replace zeros with NA
temp_df[temp_df == 0] <- NA

filtered_names <- data.frame(filtered_names, temp_df)

# Find the scientists with the shortest and longest biographies for each research field
shortest_biography <- filtered_names %>%
  group_by(Field) %>%
  filter(len_bios == min(len_bios,na.rm=TRUE))

longest_biography <- filtered_names %>%
  group_by(Field) %>%
  filter(len_bios == max(len_bios,na.rm=TRUE)) %>%
  dplyr::select(Field, Names, len_bios)

shortest_biography
longest_biography

```


## 1.5 Analysis


The code provided retrieves the URLs of scientists' Wikipedia pages, retrieves the HTML content of each page, parses the biographical text from the HTML, and stores the parsed text and its length. It handles errors by assigning NA values when parsing fails. The code then combines the parsed biographies with the original data frame, calculates the shortest and longest biographies for each research field, and displays the resulting information. Overall, the code automates the process of extracting and analyzing biographical information from scientists' Wikipedia pages.

We use the following steps :

First we creates an empty list called bios and Len_bios to store the biographies and their lengths, respectively.

That it loops over the URLs of the scientists  Wikipedia pages stored in the  URL_scientist variable.
For each URL, it tries to retrieve the HTML text of the scientist's Wikipedia page using the get_html_text() function.

It then parses the biographical text from the HTML using a custom function wiki_text_parser(), which extracts the relevant biographical information and returns a list containing the parsed text and its length.

The parsed biography text and its length are stored in the bios and Len_bios lists, respectively.




Q1.6. 
```{r, cache=TRUE} 
# Concatenate all biographies into a single text
all_biographies <- paste(bios, collapse = " ")

# Convert the text to lowercase
all_biographies <- tolower(all_biographies)

# Remove non-alphabetic characters
all_biographies <- gsub("[^a-z]", "", all_biographies)

# Count the frequency of each letter
letter_count <- table(strsplit(all_biographies, "")[[1]])
total_letters <- sum(letter_count)

# Normalize the frequencies
normalized_frequencies <- letter_count / total_letters

# Sort the normalized frequencies
sorted_frequencies <- sort(normalized_frequencies, decreasing = TRUE)

# Extract the letters and frequencies for plotting
letters <- names(sorted_frequencies)
frequencies <- sorted_frequencies

# Create a color palette for the bar plot
color_palette <- rainbow(length(letters))

# Create a data frame with letters and frequencies, sorted by frequencies
dfgraph <- data.frame(letters = letters, frequencies = frequencies)
dfgraph <- arrange(dfgraph, desc(frequencies))

# Convert the letters column to a factor with the desired order
dfgraph$letters <- factor(dfgraph$letters, levels = dfgraph$letters)

# Create a ggplot bar plot
ggplot(dfgraph, aes(x = letters, y = frequencies, fill = letters)) +
  geom_bar(stat = "identity") +
  labs(x = "Letters", y = "Normalized Frequencies",
       title = "Frequency of Letters in Concatenated Biographies") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

## 1.6 Analysis

Here we concatenates all the biographies stored in the bios list into a single text by using the paste() function, with the separator " " (space) specified as the collapse argument. 
The result is stored in the variable all_biographies.

After that we :
converts the text in all_biographies to lowercase using the to lower() function

removes non-alphabetic characters from all_biographies using the g sub() function.

The regular expression [a-z] matches any character that is not a lowercase letter from 'a' to 'z'. This step effectively removes any non-alphabetic characters from the text.

Than we did calculation to the letters and normalizes the frequencies by dividing letter_count by total_letters and stores the result in the variable normalized_frequencies .

For summary :
this code takes the concatenated biographies, calculates the frequency of each letter, normalizes the frequencies, and then generates a bar plot to visualize the sorted frequencies, with different colors bars representing each letter.


